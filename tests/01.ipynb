{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Generating 10 examples for task: As an advanced ...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Completed\n"
     ]
    }
   ],
   "source": [
    "from prompt_autotune import TunePrompt\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "prompt = \"\"\"Given an excerpt from a PDF, if you see contents listed with the page number in a table format, output the title of the contents and the start page and the end page of the contents.\n",
    "Example:\n",
    "1.1.5 Heading 1, 5, 5\n",
    "1.1.6 Heading 2, 6, 6\n",
    "1.1.7 Heading 3, 7, 8\n",
    "where the first column is the title of the contents along with the unique identifier, the second column is the start page of the contents, and the third column is the end page of the contents.\n",
    "Excerpt:\"\"\"\n",
    "\n",
    "task = \"\"\"As an advanced language model, your job is to extract every single entry in the table of contents section of document. You will be given a chunk of the Table of Contents at a time, and you will need to extract the title of the contents, the start page, and the end page of the contents.\n",
    "You shouldn't miss anything. Get all the entries at all the hirearchical levels.\"\"\"\n",
    "\n",
    "# Create a new instance of TunePrompt\n",
    "tune = TunePrompt(\n",
    "    prompt=prompt,\n",
    "    task=task,\n",
    "    verbose=True,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "    powerllm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_autotune.GenerateExamples import Example\n",
    "\n",
    "examples = []\n",
    "# exampes are different tupes of edge cases in a table of contents\n",
    "# example where the table of contents is in the format of \"1.1.5 Heading 1, 5, 5\"\n",
    "examples.append(\n",
    "    Example(\n",
    "        input=\"1.1.5 Heading 1, 5, 5\\n1.1.6 Heading 2, 6, 6\\n1.1.7 Heading 3, 7, 8\",\n",
    "        output=\"Heading 1, 5, 5\\nHeading 2, 6, 6\\nHeading 3, 7, 8\",\n",
    "    )\n",
    ")\n",
    "# example where the table of contents is in the format of \"1.1.5 Heading 1, 5-5\"\n",
    "examples.append(\n",
    "    Example(\n",
    "        input=\"1.1.5 Heading 1, 5-5\\n1.1.6 Heading 2, 6-6\\n1.1.7 Heading 3, 7-8\",\n",
    "        output=\"Heading 1, 5, 5\\nHeading 2, 6, 6\\nHeading 3, 7, 8\",\n",
    "    )\n",
    ")\n",
    "# example where the table of contents is in the format of \"Section 1, 5-5\"\n",
    "examples.append(\n",
    "    Example(\n",
    "        input=\"Section 1, 5-5\\nSection 2, 6-6\\nSection 3, 7-8\",\n",
    "        output=\"Section 1, 5, 5\\nSection 2, 6, 6\\nSection 3, 7, 8\",\n",
    "    )\n",
    ")\n",
    "# example where the table of contents is in a hireralchical format, e.g. \"I. Heading 1, 5-5\\nA. Heading 2, 6-6\\n1. Heading 3, 7-8\"\n",
    "examples.append(\n",
    "    Example(\n",
    "        input=\"I. Heading 1, 5-5\\nA. Heading 2, 6-6\\n1. Heading 3, 7-8\",\n",
    "        output=\"Heading 1, 5, 5\\nHeading 2, 6, 6\\nHeading 3, 7, 8\",\n",
    "    )\n",
    ")\n",
    "# example where the table of contents is not even in a table format, e.g. \"Heading 1, 5-5\\nHeading 2, 6-6\\nHeading 3, 7-8\"\n",
    "examples.append(\n",
    "    Example(\n",
    "        input=\"Heading 1, 5-5\\nHeading 2, 6-6\\nHeading 3, 7-8\",\n",
    "        output=\"Heading 1, 5, 5\\nHeading 2, 6, 6\\nHeading 3, 7, 8\",\n",
    "    )\n",
    ")\n",
    "# example where the table of contents is in a format of \"1.1.5 Heading 1, 5-5\\n1.1.6 Heading 2, 6-6\\n1.1.7 Heading 3, 7-8\"\n",
    "examples.append(\n",
    "    Example(\n",
    "        input=\"1.1.5 Heading 1, 5-5\\n1.1.6 Heading 2, 6-6\\n1.1.7 Heading 3, 7-8\",\n",
    "        output=\"Heading 1, 5, 5\\nHeading 2, 6, 6\\nHeading 3, 7, 8\",\n",
    "    )\n",
    ")\n",
    "\n",
    "tune.examples = examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: None\n",
      "Input: 1.1.5 Heading 1, 5, 5\n",
      "1.1.6 Heading 2, 6, 6\n",
      "1.1.7 Heading 3, 7, 8\n",
      "Output: Heading 1, 5, 5\n",
      "Heading 2, 6, 6\n",
      "Heading 3, 7, 8\n",
      "Example: None\n",
      "Input: 1.1.5 Heading 1, 5-5\n",
      "1.1.6 Heading 2, 6-6\n",
      "1.1.7 Heading 3, 7-8\n",
      "Output: Heading 1, 5, 5\n",
      "Heading 2, 6, 6\n",
      "Heading 3, 7, 8\n",
      "Example: None\n",
      "Input: Section 1, 5-5\n",
      "Section 2, 6-6\n",
      "Section 3, 7-8\n",
      "Output: Section 1, 5, 5\n",
      "Section 2, 6, 6\n",
      "Section 3, 7, 8\n",
      "Example: None\n",
      "Input: I. Heading 1, 5-5\n",
      "A. Heading 2, 6-6\n",
      "1. Heading 3, 7-8\n",
      "Output: Heading 1, 5, 5\n",
      "Heading 2, 6, 6\n",
      "Heading 3, 7, 8\n",
      "Example: None\n",
      "Input: Heading 1, 5-5\n",
      "Heading 2, 6-6\n",
      "Heading 3, 7-8\n",
      "Output: Heading 1, 5, 5\n",
      "Heading 2, 6, 6\n",
      "Heading 3, 7, 8\n",
      "Example: None\n",
      "Input: 1.1.5 Heading 1, 5-5\n",
      "1.1.6 Heading 2, 6-6\n",
      "1.1.7 Heading 3, 7-8\n",
      "Output: Heading 1, 5, 5\n",
      "Heading 2, 6, 6\n",
      "Heading 3, 7, 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses for cycle 0\n",
      "INFO:prompt_autotune.TunePrompt:Generating responses for cycle 0\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 17%|█▋        | 1/6 [00:01<00:07,  1.47s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 50%|█████     | 3/6 [00:04<00:05,  1.80s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 67%|██████▋   | 4/6 [00:06<00:02,  1.50s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 83%|████████▎ | 5/6 [00:07<00:01,  1.43s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.66s/it]\n",
      "Evaluating responses for cycle 0\n",
      "INFO:prompt_autotune.TunePrompt:Evaluating responses for cycle 0\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Old prompt: Given an excerp...\n",
      "INFO:prompt_autotune.TunePrompt:Old prompt: Given an excerp...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "New prompt: Given an excerp...\n",
      "INFO:prompt_autotune.TunePrompt:New prompt: Given an excerp...\n",
      "Generating responses for cycle 1\n",
      "INFO:prompt_autotune.TunePrompt:Generating responses for cycle 1\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 17%|█▋        | 1/6 [00:01<00:08,  1.73s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 2/6 [00:02<00:05,  1.44s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 50%|█████     | 3/6 [00:05<00:06,  2.14s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 67%|██████▋   | 4/6 [00:07<00:03,  1.78s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 83%|████████▎ | 5/6 [00:08<00:01,  1.59s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.62s/it]\n",
      "Evaluating responses for cycle 1\n",
      "INFO:prompt_autotune.TunePrompt:Evaluating responses for cycle 1\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Old prompt: Given an excerp...\n",
      "INFO:prompt_autotune.TunePrompt:Old prompt: Given an excerp...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "New prompt: Given a chunk o...\n",
      "INFO:prompt_autotune.TunePrompt:New prompt: Given a chunk o...\n",
      "Generating responses for cycle 2\n",
      "INFO:prompt_autotune.TunePrompt:Generating responses for cycle 2\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 17%|█▋        | 1/6 [00:01<00:05,  1.15s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 2/6 [00:02<00:04,  1.02s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 50%|█████     | 3/6 [00:03<00:03,  1.04s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 67%|██████▋   | 4/6 [00:03<00:01,  1.05it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 83%|████████▎ | 5/6 [00:05<00:01,  1.05s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 6/6 [00:06<00:00,  1.00s/it]\n",
      "Evaluating responses for cycle 2\n",
      "INFO:prompt_autotune.TunePrompt:Evaluating responses for cycle 2\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Old prompt: Given a chunk o...\n",
      "INFO:prompt_autotune.TunePrompt:Old prompt: Given a chunk o...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "New prompt: Given a chunk o...\n",
      "INFO:prompt_autotune.TunePrompt:New prompt: Given a chunk o...\n"
     ]
    }
   ],
   "source": [
    "tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Given a chunk of the Table of Contents from a PDF, extract the title of the contents, the start page, and the end page of each entry in a consistent format. Ensure that the page ranges are standardized with hyphens. Differentiate between hierarchical and non-hierarchical entries. Support alphanumeric characters and Roman numerals in entries. Skip or flag missing/incomplete entries for review. Handle nested levels by indenting or numbering sub-level entries for clarity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(tune.prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from prompt_autotune import TunePrompt\n",
    "\n",
    "prompt = \"\"\"Reply with 'yes' if the attached excerpt from a PDF contains list of contents, and 'no' otherwise.\n",
    "Excerpt:\"\"\"\n",
    "\n",
    "task = \"\"\"As an advanced language model, your job is to determine if the given excerpt from a PDF contains a list of contents. You will be given a chunk of text from a PDF, and you will need to determine if the text contains a list of contents. If it does, you should reply with 'yes', and if it doesn't, you should reply with 'no'.\"\"\"\n",
    "\n",
    "# Create a new instance of TunePrompt\n",
    "tune = TunePrompt(\n",
    "    prompt=prompt,\n",
    "    task=task,\n",
    "    verbose=True,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "    powerllm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    ")\n",
    "\n",
    "from prompt_autotune.GenerateExamples import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: None\n",
      "Input: 1.1.5 Heading 1, 5-5\n",
      "1.1.6 Heading 2, 6-6\n",
      "1.1.7 Heading 3, 7-8\n",
      "Output: yes\n",
      "Example: None\n",
      "Input: Section 1, 5-5\n",
      "Section 2, 6-6\n",
      "Section 3, 7-8\n",
      "Output: yes\n",
      "Example: None\n",
      "Input: I. Heading 1, 5-5\n",
      "A. Heading 2, 6-6\n",
      "1. Heading 3, 7-8\n",
      "Output: yes\n",
      "Example: None\n",
      "Input: Heading 1, 5-5\n",
      "Heading 2, 6-6\n",
      "Heading 3, 7-8\n",
      "Output: no\n",
      "Example: None\n",
      "Input: 1.1.5 Heading 1, 5-5\n",
      "1.1.6 Heading 2, 6-6\n",
      "1.1.7 Heading 3, 7-8\n",
      "Output: yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses for cycle 0\n",
      "INFO:prompt_autotune.TunePrompt:Generating responses for cycle 0\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.15it/s]\n",
      "Evaluating responses for cycle 0\n",
      "INFO:prompt_autotune.TunePrompt:Evaluating responses for cycle 0\n",
      "Old prompt: Reply with 'yes...\n",
      "INFO:prompt_autotune.TunePrompt:Old prompt: Reply with 'yes...\n",
      "New prompt: Reply with 'yes...\n",
      "INFO:prompt_autotune.TunePrompt:New prompt: Reply with 'yes...\n",
      "Generating responses for cycle 1\n",
      "INFO:prompt_autotune.TunePrompt:Generating responses for cycle 1\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.89it/s]\n",
      "Evaluating responses for cycle 1\n",
      "INFO:prompt_autotune.TunePrompt:Evaluating responses for cycle 1\n",
      "Old prompt: Reply with 'yes...\n",
      "INFO:prompt_autotune.TunePrompt:Old prompt: Reply with 'yes...\n",
      "New prompt: Reply with 'yes...\n",
      "INFO:prompt_autotune.TunePrompt:New prompt: Reply with 'yes...\n",
      "Generating responses for cycle 2\n",
      "INFO:prompt_autotune.TunePrompt:Generating responses for cycle 2\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.36it/s]\n",
      "Evaluating responses for cycle 2\n",
      "INFO:prompt_autotune.TunePrompt:Evaluating responses for cycle 2\n",
      "Old prompt: Reply with 'yes...\n",
      "INFO:prompt_autotune.TunePrompt:Old prompt: Reply with 'yes...\n",
      "New prompt: Reply with 'yes...\n",
      "INFO:prompt_autotune.TunePrompt:New prompt: Reply with 'yes...\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "# examples of a table of contents\n",
    "examples.append(Example(input=\"1.1.5 Heading 1, 5-5\\n1.1.6 Heading 2, 6-6\\n1.1.7 Heading 3, 7-8\", output=\"yes\"))\n",
    "examples.append(Example(input=\"Section 1, 5-5\\nSection 2, 6-6\\nSection 3, 7-8\", output=\"yes\"))\n",
    "examples.append(Example(input=\"I. Heading 1, 5-5\\nA. Heading 2, 6-6\\n1. Heading 3, 7-8\", output=\"yes\"))\n",
    "examples.append(Example(input=\"Heading 1, 5-5\\nHeading 2, 6-6\\nHeading 3, 7-8\", output=\"no\"))\n",
    "examples.append(Example(input=\"1.1.5 Heading 1, 5-5\\n1.1.6 Heading 2, 6-6\\n1.1.7 Heading 3, 7-8\", output=\"yes\"))\n",
    "\n",
    "tune.examples = examples\n",
    "\n",
    "tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reply with 'yes' if the provided text exhibits characteristics commonly found in lists of contents, such as variations in numbering or lettering formats like numbers, letters, Roman numerals, etc., or includes keywords like \"Table of Contents\" or \"Contents\". Consider hierarchical structures, patterns, and mixed formats of list items to distinguish between regular headings and a list of contents. If unsure, provide feedback on why the text may or may not be a list of contents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(tune.prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
